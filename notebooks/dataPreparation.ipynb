{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "OpenCV loads images in BGR format, while Matplotlib expects RGB format when displaying images using `plt.imshow()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"..\\data\\Semantic segmentation dataset\\Tile 1\\images\\image_part_001.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(image.shape)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "\n",
    "mask = cv2.imread(r\"..\\data\\Semantic segmentation dataset\\Tile 1\\masks\\image_part_001.png\")\n",
    "mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "print(mask.shape)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "crops the image using NumPy slicing to ensure that its dimensions are exact multiples of IMAGE_PATCH_SIZE\n",
    "\n",
    "Another way to crop the image using the `PIL` package:\n",
    "```py\n",
    "image = Image.fromarray(image)\n",
    "image = image.crop((0, 0, size_x, size_y))\n",
    "image = np.array(image)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"..\\data\\Semantic segmentation dataset\\Tile 1\\images\\image_part_001.jpg\")\n",
    "print(image.shape)\n",
    "\n",
    "size_x = (image.shape[1] // IMAGE_PATCH_SIZE) * IMAGE_PATCH_SIZE\n",
    "size_y = (image.shape[0] // IMAGE_PATCH_SIZE) * IMAGE_PATCH_SIZE\n",
    "\n",
    "# crop the image to be multiples of IMAGE_PATCH_SIZE\n",
    "image = image[0:size_y, 0:size_x]\n",
    "print(image.shape)\n",
    "\n",
    "patched_images = patchify(image, (IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE, 3), step=IMAGE_PATCH_SIZE)\n",
    "print(patched_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(patched_images.shape[0]):\n",
    "\tfor j in range(patched_images.shape[1]):\n",
    "\t\tprint(patched_images[i, j, 0].shape)\n",
    "\n",
    "len(patched_images.reshape(-1, IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "l.extend(patched_images.reshape(-1, IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE, 3))\n",
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "minMaxScaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = []\n",
    "mask_dataset = []\n",
    "\n",
    "for image_type, extension in [('images', 'jpg'), ('masks', 'png')]:\n",
    "\tfor tile_idx in range(8):\n",
    "\t\tfor image_idx in range(9):\n",
    "\t\t\timage = cv2.imread(fr\"..\\data\\Semantic segmentation dataset\\Tile {tile_idx+1}\\{image_type}\\image_part_00{image_idx+1}.{extension}\")\n",
    "\t\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\t\t\tsize_x = (image.shape[1] // IMAGE_PATCH_SIZE) * IMAGE_PATCH_SIZE\n",
    "\t\t\tsize_y = (image.shape[0] // IMAGE_PATCH_SIZE) * IMAGE_PATCH_SIZE\n",
    "\t\t\timage = image[0:size_y, 0:size_x]\n",
    "\t\t\tpatched_images = patchify(image, (IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE, 3), step=IMAGE_PATCH_SIZE)\n",
    "\t\t\tfor i in range(patched_images.shape[0]):\n",
    "\t\t\t\tfor j in range(patched_images.shape[1]):\n",
    "\t\t\t\t\tindividual_patched_image = patched_images[i, j, 0]\n",
    "\t\t\t\t\tif image_type == 'images':\n",
    "\t\t\t\t\t\tindividual_patched_image = minMaxScaler.fit_transform(individual_patched_image.reshape(-1, individual_patched_image.shape[-1])).reshape(individual_patched_image.shape)\n",
    "\t\t\t\t\t\timage_dataset.append(individual_patched_image)\n",
    "\t\t\t\t\telif image_type == 'masks':\n",
    "\t\t\t\t\t\tmask_dataset.append(individual_patched_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(image_dataset))\n",
    "print(len(mask_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = np.array(image_dataset)\n",
    "mask_dataset = np.array(mask_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "let's display the 1st patch of the 1st tile, that we displayed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_dataset[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask_dataset[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"..\\data\\Semantic segmentation dataset\\classes.json\") as f_in:\n",
    "\tclasses_file = json.load(f_in)\n",
    "\n",
    "print(type(classes_file))\n",
    "classes_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "| class | actual colors on mask images | colors in classes.json file (contains mistakes) |\n",
    "| ----- | ----- | ----- |\n",
    "| Building: | #3C1098 | #D0021B |\n",
    "| Land (unpaved area): | #8429F6 | #F5A623 |\n",
    "| Road: | #6EC1E4 | #DE597F |\n",
    "| Vegetation: | #FEDD3A | #417505 |\n",
    "| Water: | #E2A929 | #50E3C2 |\n",
    "| Unlabeled: | #9B9B9B | #9B9B9B |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_file['classes'][0]['color'] = '#E2A929'\n",
    "classes_file['classes'][1]['color'] = '#8429F6'\n",
    "classes_file['classes'][2]['color'] = '#6EC1E4'\n",
    "classes_file['classes'][3]['color'] = '#3C1098'\n",
    "classes_file['classes'][4]['color'] = '#FEDD3A'\n",
    "classes_file['classes'][5]['color'] = '#9B9B9B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the new edited classes.json (overwrite the old one)\n",
    "with open(r\"..\\data\\Semantic segmentation dataset\\classes.json\", 'w') as f_out:\n",
    "\tjson.dump(classes_file, f_out, indent=4)  # use indent for pretty-printing (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label: color → e.g., 0 is the label of the 'Water' class\n",
    "classes = dict()\n",
    "for label, cls in enumerate(classes_file['classes']):\n",
    "    hex_color = cls['color'].lstrip('#')\n",
    "    classes[label] = np.array([int(hex_color[i:i+2], 16) for i in (0, 2, 4)])\n",
    "    \n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Mask images (labels) are RGB images, where each class is represented by a unique color (e.g., Building = #3C1098 → dark purple). However, neural networks don't understand colors; they require integer class IDs, such as Water = 0. So, we need to convert RGB masks to label IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_label(mask):\n",
    "    label_segment = np.zeros(mask.shape, dtype=np.uint8)\n",
    "    for label, rgb in classes.items():\n",
    "        label_segment[np.all(mask == rgb, axis=-1)] = label\n",
    "    # removes extra channels (since it's now just label IDs, we only need 2D, not 3D)\n",
    "    label_segment = label_segment[:, :, 0] # the last index can be 0, 1, or 2 as they're all the same\n",
    "    return label_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(mask_dataset.shape[0]):\n",
    "    label = rgb_to_label(mask_dataset[i])\n",
    "    labels.append(label)\n",
    "    \n",
    "labels = np.array(labels)\n",
    "print(labels.shape) # (n_samples, H, W)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_dataset[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(labels[0])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.expand_dims(labels, axis=3)\n",
    "print(labels.shape)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Now shape is (n_samples, H, W, 1), adding a dummy channel so Keras doesn't complain (expects channel-last input). So, that's expected by Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "For **Focal** and **Dice** losses, we must use one-hot encoded labels. If we need to use sparse_categorical_crossentropy, we must ignore the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "labels_categorical_dataset = to_categorical(labels, num_classes=num_classes)\n",
    "labels_categorical_dataset.shape\n",
    "# Now shape is (n_samples, H, W, num_classes) → one-hot encoded masks. Required for softmax + Focal, Dice, and categorical_crossentropy losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_categorical_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, labels_categorical_dataset, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump((X_train, X_test, y_train, y_test), r'..\\data\\dataset.joblib', compress=9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
