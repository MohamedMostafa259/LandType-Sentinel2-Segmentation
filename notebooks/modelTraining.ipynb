{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THE FOLLOWING IF YOU ARE USING KAGGLE\n",
    "# !pip install segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THE FOLLOWING IF YOU ARE USING KAGGLE\n",
    "# import os\n",
    "# os.environ['SM_FRAMEWORK'] = 'tf.keras'\n",
    "\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import numpy as np\n",
    "import segmentation_models as sm\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "if not hasattr(K, 'sigmoid'):\n",
    "    K.sigmoid = tf.nn.sigmoid\n",
    "\n",
    "# UNCOMMENT THE FOLLOWING IF YOU ARE USING KAGGLE\n",
    "# sm.set_framework('tf.keras')\n",
    "# print(\"Framework set to:\", sm.framework())\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "IMAGE_PATCH_SIZE = 256\n",
    "NUM_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['accuracy', sm.metrics.IOUScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"U-Net Architecture\" style=\"width: 70%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Divergence from the original U-Net (2015) a little bit\n",
    "\n",
    "- We're adding **dropout** to avoid overfitting; however, it doesn't exist in the original U-Net (2015).\n",
    "\t- Dropout rates increase deeper in the encoder: 0.1 → 0.3, because deeper layers tend to overfit more.\n",
    "\n",
    "- We're `padding='same'`; however, the original U-Net used 'valid' padding but also performed manual cropping before concatenation to make dimensions match.\n",
    "\n",
    "- Reduced the number of filters on each step, as the original architecture needs a very powerful GPU (more powerful than Kaggle's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom Conv-BN-ReLU-Dropout block\n",
    "def ConvBNDropout(x, filters, dropout_rate=0.1):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=3, padding='same', \n",
    "                               kernel_initializer='he_normal', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "# custom Transposed Convolution block \n",
    "def Upsample(filters, x):\n",
    "    return tf.keras.layers.Conv2DTranspose(filters, kernel_size=2, strides=2, \n",
    "                                  padding='same', kernel_initializer='he_normal')(x)\n",
    "\n",
    "\n",
    "def build_unet(input_shape=(IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE, NUM_CHANNELS), \n",
    "               dropout_rate=0.1, num_classes=NUM_CLASSES):\n",
    "    \n",
    "    input = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = ConvBNDropout(input, 32, dropout_rate)\n",
    "    c1 = ConvBNDropout(c1, 32, dropout_rate)\n",
    "    p1 = tf.keras.layers.MaxPooling2D()(c1)\n",
    "\n",
    "    c2 = ConvBNDropout(p1, 64, dropout_rate)\n",
    "    c2 = ConvBNDropout(c2, 64, dropout_rate)\n",
    "    p2 = tf.keras.layers.MaxPooling2D()(c2)\n",
    "\n",
    "    c3 = ConvBNDropout(p2, 128, dropout_rate)\n",
    "    c3 = ConvBNDropout(c3, 128, dropout_rate)\n",
    "    p3 = tf.keras.layers.MaxPooling2D()(c3)\n",
    "\n",
    "    c4 = ConvBNDropout(p3, 256, dropout_rate)\n",
    "    c4 = ConvBNDropout(c4, 256, dropout_rate)\n",
    "    p4 = tf.keras.layers.MaxPooling2D()(c4)\n",
    "\n",
    "    # Bottleneck\n",
    "    c5 = ConvBNDropout(p4, 512, dropout_rate)\n",
    "    c5 = ConvBNDropout(c5, 512, dropout_rate)\n",
    "\n",
    "    # Decoder\n",
    "    u6 = Upsample(256, c5)\n",
    "    u6 = tf.keras.layers.Concatenate()([u6, c4])\n",
    "    c6 = ConvBNDropout(u6, 256, dropout_rate)\n",
    "    c6 = ConvBNDropout(c6, 256, dropout_rate)\n",
    "\n",
    "    u7 = Upsample(128, c6)\n",
    "    u7 = tf.keras.layers.Concatenate()([u7, c3])\n",
    "    c7 = ConvBNDropout(u7, 128, dropout_rate)\n",
    "    c7 = ConvBNDropout(c7, 128, dropout_rate)\n",
    "\n",
    "    u8 = Upsample(64, c7)\n",
    "    u8 = tf.keras.layers.Concatenate()([u8, c2])\n",
    "    c8 = ConvBNDropout(u8, 64, dropout_rate)\n",
    "    c8 = ConvBNDropout(c8, 64, dropout_rate)\n",
    "\n",
    "    u9 = Upsample(32, c8)\n",
    "    u9 = tf.keras.layers.Concatenate()([u9, c1])\n",
    "    c9 = ConvBNDropout(u9, 32, dropout_rate)\n",
    "    c9 = ConvBNDropout(c9, 32, dropout_rate)\n",
    "\n",
    "\t# each pixel has a vector of probabilities over all classes; that's why we have num_classes feature maps\n",
    "    output = tf.keras.layers.Conv2D(num_classes, kernel_size=1, activation='softmax')(c9)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input], outputs=[output])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = joblib.load(r'..\\data\\dataset.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, \n",
    "                                                  patience=10)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                                                 patience=3)\n",
    "\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "##### Plot model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.py in the root dir\n",
    "class PlotDiagnostics(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.epoch_count = 0\n",
    "        self.x = []\n",
    "\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "        self.iou_score = []\n",
    "        self.val_iou_score = []\n",
    "\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.epoch_count)\n",
    "\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "\n",
    "        self.iou_score.append(logs.get('iou_score'))\n",
    "        self.val_iou_score.append(logs.get('val_iou_score'))\n",
    "\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "\n",
    "        self.epoch_count += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(18, 5))\n",
    "        f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5), sharex=True)\n",
    "\n",
    "        # Plot Loss\n",
    "        ax1.plot(self.x, self.losses, label=\"loss\")\n",
    "        ax1.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        ax1.set_title(\"Loss\")\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot IoU (Jaccard Index)\n",
    "        ax2.plot(self.x, self.iou_score, label=\"iou_score\")\n",
    "        ax2.plot(self.x, self.val_iou_score, label=\"val_iou_score\")\n",
    "        ax2.set_title(\"IoU Score\")\n",
    "        # ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "\n",
    "        # Plot Accuracy\n",
    "        ax3.plot(self.x, self.accuracy, label=\"accuracy\")\n",
    "        ax3.plot(self.x, self.val_accuracy, label=\"val_accuracy\")\n",
    "        ax3.set_title(\"Accuracy\")\n",
    "        ax3.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_diagnostics = PlotDiagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[plot_diagnostics, early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Comparing prediction results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred.shape)\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "`matplotlib.pyplot.imshow()` assigns colors automatically based on the values. So even if y_test and y_pred have the same values, the colors may differ. It doesn’t know which color corresponds to which class unless you force it to using a colormap.\n",
    "\n",
    "With the fixed `ListedColormap`, both the ground truth and predicted images will show the same colors for the same classes, making visual comparison meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\n",
    "    '#E2A929',  # Class 0\n",
    "    '#8429F6',  # Class 1\n",
    "    '#6EC1E4',  # Class 2\n",
    "    '#3C1098',  # Class 3\n",
    "    '#FEDD3A',  # Class 4\n",
    "    '#9B9B9B'   # Class 5\n",
    "]\n",
    "\n",
    "cmap = ListedColormap(color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 9, 4): \n",
    "    original_image = X_test[i] \n",
    "    ground_truth_image = y_test[i] \n",
    "    predicted_image = y_pred[i] \n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1) \n",
    "    plt.title(\"Original Image\") \n",
    "    plt.imshow(original_image) \n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2) \n",
    "    plt.title(\"Ground Truth\") \n",
    "    plt.imshow(ground_truth_image, cmap=cmap, vmin=0, vmax=len(color_list) - 1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3) \n",
    "    plt.title(\"Prediction\") \n",
    "    plt.imshow(predicted_image, cmap=cmap, vmin=0, vmax=len(color_list) - 1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"satellite_segmentation_full.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Create segmentation model with pretrained encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = joblib.load(r'../data/dataset.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet('efficientnetb4', input_shape=(IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE, NUM_CHANNELS), \n",
    "                classes=NUM_CLASSES, activation='softmax', encoder_weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, \n",
    "                                                  patience=10)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, \n",
    "                                                 patience=3)\n",
    "\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[plot_diagnostics, early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "It's clear that the new pretrained model (accuracy of \"0.89\" and IoU of \"0.68\") is better than the model I've created from scratch (accuracy of \"0.84\" and IoU of \"0.55\"). So, let's use this pretrained model instead when deploying to hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y_test, axis=-1)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "color_list = [\n",
    "    '#E2A929',  # Class 0\n",
    "    '#8429F6',  # Class 1\n",
    "    '#6EC1E4',  # Class 2\n",
    "    '#3C1098',  # Class 3\n",
    "    '#FEDD3A',  # Class 4\n",
    "    '#9B9B9B'   # Class 5\n",
    "]\n",
    "cmap = ListedColormap(color_list)\n",
    "\n",
    "for i in range(0, 9, 4): \n",
    "    original_image = X_test[i] \n",
    "    ground_truth_image = y_test[i] \n",
    "    predicted_image = y_pred[i] \n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1) \n",
    "    plt.title(\"Original Image\") \n",
    "    plt.imshow(original_image) \n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2) \n",
    "    plt.title(\"Ground Truth\") \n",
    "    plt.imshow(ground_truth_image, cmap=cmap, vmin=0, vmax=len(color_list) - 1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3) \n",
    "    plt.title(\"Prediction\") \n",
    "    plt.imshow(predicted_image, cmap=cmap, vmin=0, vmax=len(color_list) - 1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n",
    "\n",
    "model.save(\"satellite_segmentation_model_pretraining.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Performing prediction using an image from Google Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    '../models/satellite_segmentation_model_pretraining.keras',\n",
    "    compile=False)\n",
    "\n",
    "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\n",
    "    '#E2A929',  # Class 0\n",
    "    '#8429F6',  # Class 1\n",
    "    '#6EC1E4',  # Class 2\n",
    "    '#3C1098',  # Class 3\n",
    "    '#FEDD3A',  # Class 4\n",
    "    '#9B9B9B'   # Class 5\n",
    "]\n",
    "cmap = ListedColormap(color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://project.inria.fr/aerialimagelabeling/\n",
    "image = Image.open(r\"..\\visulas\\test\\test2_InriaAerialImageLabeling.jpg\")\n",
    "image = image.resize((IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE))\n",
    "image = np.array(image)[:, :, :3] # keep only RGB; drop alpha channel\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  \n",
    "image = np.expand_dims(image, axis=0) # batch of size 1\n",
    "image = image / 255.0\n",
    "\n",
    "predicted_image = model.predict(image)\n",
    "predicted_image = np.argmax(predicted_image, axis=-1)\n",
    "predicted_image = predicted_image[0, ...]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_image, cmap=cmap, vmin=0, vmax=len(color_list) - 1)   \n",
    "plt.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://phys.org/news/2012-08-satellite-view-house.html\n",
    "image = Image.open(r\"..\\visulas\\test\\test1_GoogleMaps.png\")\n",
    "image = image.resize((IMAGE_PATCH_SIZE, IMAGE_PATCH_SIZE))\n",
    "image = np.array(image)[:, :, :3] # keep only RGB; drop alpha channel\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  \n",
    "image = np.expand_dims(image, axis=0) # batch of size 1\n",
    "image = image / 255.0\n",
    "\n",
    "predicted_image = model.predict(image)\n",
    "predicted_image = np.argmax(predicted_image, axis=-1)\n",
    "predicted_image = predicted_image[0, ...]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(predicted_image, cmap=cmap, vmin=0, vmax=len(color_list) - 1)   \n",
    "plt.axis('off') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7938041,
     "sourceId": 12570032,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
